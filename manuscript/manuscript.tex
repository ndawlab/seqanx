\documentclass[11pt]{article} % For LaTeX2e
\usepackage{manuscript, palatino}
\usepackage{graphicx}
\usepackage{amsfonts, amsmath}
\usepackage{algorithm, algpseudocode}%

\usepackage{natbib}

\title{Title Pending}

\author{
Samuel Zorowitz \\
Princeton Neuroscience Institute\\
Princeton University\\
Princeton, NJ 08540 \\
\texttt{zorowitz@princeton.edu} \\
\And
Ida Momennejad \\
Columbia University\\
New York, NY 10027 \\
\texttt{ida.m@columbia.edu} \\
\And
Nathaniel Daw \\
Princeton Neuroscience Institute\\
Princeton University\\
Princeton, NJ 08540 \\
\texttt{ndaw@princeton.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Pending
\end{abstract}

\keywords{
anxiety; avoidance; fear generalization; decision theory; computational psychiatry
}

\startmain

\section{Introduction}

Anxiety disorders are characterized by aberrations in the cognitive processing of and response to threat. The hallmark symptom of anxiety disorders is exaggerated threat appraisal, or the tendency to perceive threat as disproportionately greater in likelihood and/or severity than is warranted. Clinical anxiety often also involves the generalization of fear, wherein an individual comes to perceive as threatening cues or situations secondarily associated with the primary fear. For example, a dog phobic individual may first fear dog parks, then parks in general, and finally any public space where a dog may plausibly be encountered. Unsurprisingly then, anxiety disorders involve persistent avoidance behavior. Avoidance behaviors in anxiety are especially pernicious as they prevent potential exposure to disconfirming evidence about perceived threats, thereby maintaining the disorder, and because they are often highly disruptive to everyday functioning. Avoidance of perceived threat often comes at considerable expense, either in foregone rewards or in the expenditure of effort. On their walk to work, the same dog phobic individual may take increasingly circuitous paths in order to avoid certain public spaces and may abandon social obligations for fear of encountering a dog. These behaviors are puzzling to theories of learning and choice behavior, not in the least because they are still observed even in safe situations posing little to no objective threat.

Some insight into anxious cognition might possibly be gleaned from a decision theoretic perspective. In decision theory, the value of potential actions (e.g. to approach or avoid) is defined in part as a function of expectations about the future. The best action is that which yields the greatest cumulative reward, contingent on current beliefs of what will likely be encountered in the future. Interestingly, anxiety disorders are associated with pessimistic beliefs about the future; anxious individuals are more likely to endorse beliefs like that they will surely face danger in the future and be unable to respond effectively to it. Despite this common finding, most studies of anxious decision making interrogate behavior in response to immediate but not future threat. Studying anxious appraisal of future threat may be more important for understanding anxiety disorders as symptoms like fear generalization and avoidance show worry for would-be but not immediate danger.

Here, we explore these issues by offering a decision theoretic account of sequential choice under pessimistic future expectations. We consider optimal behavior in deterministic, finite Markov decision processes (MDP), in which agents learn reward-maximizing (or loss-minimizing) policies in grid world environments requiring a series of choices. We first describe a fundamental asymmetry in the differential influence of reward and danger on value estimation under normative models of learning, namely that reward propagates recursively but danger does not. We then demonstrate how this asymmetry is eliminated when agents instead learn with anxious-like expectations of future action. We find that a minor deviation from normativity captures many features of anxious behavior including exaggerated threat appraisals, fear generalization, and persistent avoidance.

In the remaining sections, we also show that this simple model can account for a number of findings reported in the minority of studies that have probed anxious behavior in sequential choice environments. We then offer a novel prediction based on the the model that, consistent with negative future expectations, anxious individuals should not exhibit a free choice premium. We then close with a discussion of the relationship of our decision theoretic account to prominent clinical theories of anxiety, potential extensions to the model, and possible neural correlates.

\section{The model}

We first briefly review the formalism of Markov decision processes, which will provide the framework for our results. For complete treatments, see \cite{SuttonBarto1998, SuttonBarto2018} and \cite{bertsekas2005}.

A MDP is defined by a set of states, $S$, a set of actions, $A$, a reward function defined over state-action pairs, $R(s,a)$, and a state transition distribution, $P(s'|s,a)$. In a MDP, states and rewards are experienced sequentially according to chosen actions and the one-step transition structure. The goal for an agent in a MDP is to learn a policy, or mapping of actions to states, that maximizes expected cumulative (discounted) reward. The value of a state can be defined recursively as the sum of the immediate reward received following an action, $R(s, a)$, and the value of its successor state $sâ€™$, averaged over potential future transitions and actions according to the current policy $\pi$. Under Bellman optimality, the value of a state under the optimal policy is equal to the expected return for the best action from that state.

$$ V^*(s) = \max_a \sum_{s',r} p(s',r|s,a) \left[ r + \gamma v(s') \right] $$

where $\gamma$ is the temporal discounting parameter, which controls the influence of distant future rewards.

Crucially then, the value of a state under the optimal policy is contingent on an agent's particular expectations about the future. First, it is a function of the state transition distribution, or beliefs about the likelihood of encountering particular successor states conditioned on performing some action. Second, it is also a function of the actions available in those successor states, as future actions in turn define their value. Under Bellman optimality, it is normative for an agent to assume that the value of a successor state is equivalent to the best action from that state. This can be expressed as:

$$ V^*(s) = \max_a \sum_{s',r}p(s',r|s,a) \left[ r + \gamma \max_{a'} q_*(s',a') \right] $$

where $q(s',a')$ denotes the a state-action value. As we will demonstrate, this assumption has profound consequences for learning about reward and danger.

In a MDP, the discovery of a rewarding action by an agent has an effect not only on the associated state but also on its antecedent states. This follows naturally from the recursive definition of state values defined above. When an agent learns a particular state predicts reward, the states which immediately precede it also acquire value (as do their antecedent states, and so on). The discovery of a rewarding action thus propagates positive value across the state space, so as to motivate the future visitation of rewarding states. A similar spread of value does not occur, however, upon the discovery of a suboptimal or harmful action. Under the optimal policy, only the best action is chosen; a reward-maximizing agent will avoid performing a harmful action insofar that another option is available. When avoidance is possible, the discovery of a harmful action will not impact the value of its associated state. Moreover, an avoidable harmful action will not influence the value of its antecedent states, as is reflected by the max operator in the equation above. Thus, the optimistic assumptions about future behavior under Bellman optimality yields a fundamental asymmetry in the treatment of reward and danger: whereas rewarding opportunity propagates recursively to antecedent states, danger does not whenever avoidance is possible.

This principle is highlight in a toy MDP, the open field environment (Figure 1a). The open field is a grid world that entirely empty except for two states, one rewarding and one harmful. Importantly, the open field MDP is deterministic, such that the one-step transition distribution respects the agent's choice. An agent in this environment then will only experience an aversive outcome if it chooses to approach the dangerous state. The state value estimates learned by an agent under Bellman optimality reflect the asymmetry described above (Figure 1b). In the open field, reward propagates recursively from the source to antecedent states whereas danger does not. Thus the agent learns to associate all states (except the harmful state itself) with positive value. Because harm is avoidable in this environment, all states (even those proximal to threat) come to represent the opportunity for reward.

What happens then when an agent believes (falsely) that danger is not avoidable? In the MDP formalism, such an errant  belief may manifest in numerous ways. First, an agent may possess inaccurate assumptions about the state transition distribution. For example, an agent may wrongly believe it possible that it could transition to a state closer to danger in spite of an avoidance action. Alternately, an agent may operate under pessimistic expectations about the availability of future actions. It may believe it has the capacity to act in accordance with the optimal (reward-maximizing) policy in the present, but may be unable to do so in the future. We note here that similar beliefs about the unreliability of self or the environment in the presence of perceived threat are often endorsed in anxiety disorders.

We can easily model pessimistic expectations about future action through a minor change to the definition of state value. Previously introduced by \cite{Gaskett2003}, we define pessimistic learning as the expectation of future deviation from the optimal policy such that the value of a successor state is equal to:

$$ V(s') = w \max_{a'} q(s',a') + (1 - w) \min_{a'} q(s',a') $$

such that the state value under the optimal policy is defined as:

$$ V(s) = \max_a \sum_{s',r}p(s',r|s,a) \left[ r + \gamma \left( w \max_{a'} q(s',a') + (1 - w) \min_{a'} q(s',a') \right) \right] $$

Here, the parameter $w$ controls the degree of pessimism. When $\w = 1$, an agent is maximally optimistic and fully expects to act according to the best possible actions in the future. When $w = 0$, the agent is maximally pessimistic and fully expects to act according to the worst possible actions in the future. When $w = 0.5$, the weighs the best and worst possible future actions equally. We note we are not committed to this particular implementation. Pessimistic learning could be introduced through a variety of other manipulations. We have selected this particular formalism because out of convenience; it does not constitute a mechanistic claim about anxious cognition.

\begin{figure}
  \centerline{%
    \resizebox{1.0\textwidth}{!}{\includegraphics[trim={0 0 0 0},clip]{../figures/01_field.png}}%
  }
  \par \textbf{Figure 1:} The open field is a deterministic gridworld with only one rewarding (blue) and aversive (red) state (A). For an optimistic agent ($w=1$), all states (other than the harmful state) take on positive value (B). For a pessimistic agent ($w=0.5$), negative value spreads from the source to antecedent states (C). With increasing pessimism ($w=0$), the extent of the spread grows worse (D). (Parameters: $\gamma = 0.95$)
\end{figure}

For a pessimistic agent ($w > 0$), the expectation that future action will always be optimal (reward-maximizing) is violated; instead, the agent believes that it may act with some probability in direct contrast to its preferences. Under these conditions, an agent cannot expect to invariably avoid choosing a harmful action. Therefore, the presence of danger now poses threat and safety cannot be guaranteed. The consequences of this manipulation for the open field environment are presented in Figure 1c,d. The deviation from Bellman optimality, wherein the value of antecedent states are contingent on both the best and worst possible future actions, means that now more than reward opportunities are able to propagate recursively through state space. Indeed when $w$ is non-zero, threat value from danger also recursively propagates from its source through adjacent states. The extent of this spread is contingent on the degree of pessimistic belief. As an agent grows increasingly certain of future deleterious action (i.e. $w \rightarrow 1$), the greater the extent to which threat value permeates the state space.

We also note that this simple demonstration produces behaviors similar to the core symptoms of anxiety disorders. As is evidence in Figure 1c,d, the pessimistic agent exhibits exaggerated threat appraisals. It estimates danger at disproportionately more states than is warranted (likelihood bias) and perceives those states as more threatening than they objectively are (severity bias). Similarly, the propagation of threat value is reminiscent of the fear generalization observed in clinical anxiety. Through pessimistic learning the agent comes to regard as threatening states with increasingly distant associations to the danger source.

The model also predicts persistent avoidance behaviors similar to those observed in clinical anxiety. Importantly, the avoidance behavior exhibited by the model begins early in the chain of sequential choices. In other words, the agent performs avoidance before it is proximal to danger. This is because agents in this model act in response to anticipated future threat. Specifically, the agent cannot be certain it will not act against its preferences in the future. To minimize the likelihood of inadvertently encountering danger, the agent therefore places as much distance between itself and the possible danger through early avoidance actions. As discussed in the introduction, this is what is observed both clinically and empirically in studies of anxious individuals responding to threat \citep{Bach2014, Bach2017, Sheynin2014}. Thus our model not only predicts persistent avoidance behavior, but also captures the excessive, overly cautious profile of avoidance behavior exhibited in clinical anxiety.

\section{Simulations}

In the following sections, we show that our model is capable of recreating the results of several empirical studies of anxious cognition in sequential choice environments. We represent these experiments again using the formalism of MDPs. In all simulations, the values of environment states under the optimal policy were solved for directly through Q-value iteration \citep{SuttonBarto1998, SuttonBarto1998, bertsekas2005}. The details of this algorithm are presented in the appendix. All simulations were carried out with the python programming language and are publicly available on Github (https://github.com/szorowi1/SecretFunTimes).

\subsection{Approach-Avoidance Conflict}

Next, we turn our attention to a related phenomenon observed in anxiety: a bias towards avoidance during approach-avoidance conflict \citep{aupperle2010}. In situations where the potential for reward and harm are correlated,
anxiety predicts the tendency to forego reward in favor of minimizing the risk of threat. As a consequence, individuals with anxiety may experience fewer rewards in their environment as compared to non-anxious individuals. For example, an individual with social anxiety may avoid a party for fear of social embarrassment at the expense of positive social interactions with friends. This is a primary mechanism by which clinical anxiety can disrupt everyday functioning.

A relationship between anxiety and biased decision making in approach-avoidance conflict is well-documented in the empirical literature. One common measure is the balloon analog risk task (BART) \citep{Lejuez2002}. The task is straightforward: a participant inflates a virtual balloon, one pump at a time. The more inflated the balloon, the more points the participant earns. With increasing pumps, however, comes the risk of the balloon popping and the loss of all earned points so far. Thus, there is a conflict: a participant can choose to pump the balloon more to earn more points, at the risk of the balloon popping; alternately, the participant can choose to end the trial earlier with fewer points earned. Previous studies have found that anxiety is correlated with earlier leaving times, or a greater tendency to avoid risk \cite{Maner2007, Giorgetta2012}.

\begin{figure}
  \centerline{%
    \resizebox{1.0\textwidth}{!}{\includegraphics[trim={0 0 0 0},clip]{../figures/03_fid.png}}%
  }
  \par \textbf{Figure 3:} The Balloon Analog Risk Task (BART). Risk of the balloon popping increases with each pump (top row). An optimistic agent ($w=1$) continues to pump until the risk of losing all points earned outweighs the marginal value of one additional pump (second row). A pessimistic agent ($w=0.8$) is less confident that it will make the optimal decision in the future, and thus leaves earlier (third row). Leaving time becomes sooner with increasing pessimism ($w=0.6$) (bottom row). (Parameters: $\gamma = 1$.)
\end{figure}

This result can be accounted for by a loss of confidence in future reliability. A simulation of performance on the BART is shown in Figure 3. For an optimistic agent ($w=1$), the optimal policy is to continue pumping until the risk of popping is all but guaranteed. The optimal policy for pessimistic agents differ. Under a loss of confidence for future action, pessimistic agents cannot be certain they will make optimal choices in the future. As such, negative value propagates backwards resulting in earlier leaving times. The latency of leaving is dependent on the strength of the pessimistic belief.

Interestingly, these results are similar to recent findings regarding flight initiation distance and anxiety \citep{Mobbs2018, Mobbs2019}. Flight initiation distance refers to a measure from behavioral ecology that assesses the leaving time of a foraging animal at risk of predation. In classic flight initiation scenarios, an animal can continue to forage and exploit resources, but the longer it does so the greater the risk it will encounter a predator. Thus, flight initiation scenarios are similar in structure to approach-avoidance conflict and the BART. Recently, \cite{Mobbs2019} found that anxiety is correlated with earlier leaving times in virtual flight initiation tasks. Interestingly, the authors make a distinction between "fast-" and "slow-attacking" predators, and find that trait anxiety is only correlated only in behavioral response to the latter. The findings here draw upon an emerging literature of distinct neural systems mediating "cognitive" vs. "reactive" threat responses, where anxiety evidently impacts the deliberative, cognitive responses. Thus, it is important to note the present theory can account for anxious approach-avoidance behavior in some, but certainly not all, threatening scenarios.

These results may also be related to recent work on the computational mechanisms underlying behavioral inhibition, a temperament linked to anxiety \citep{bach2015, khemka2017}. \cite{bach2015} argues that delayed behavioral responses under threat are normative under the assumption that a pause in behavior (e.g. freezing) can be useful for surveying a threatening environment, avoiding detection by predators, and potentially waiting out a nearby threat. Behavioral inhibition, it is argued, is a useful Pavlovian heuristic that has evolved over time. The current models may predict similar results. Insofar that a loss of confidence in future action yields more negative estimates of action values, then the best action may often be brought closer in value to its alternatives. As such, a pessimistic agent may face increased decision conflict as compared to a non-anxious counterpart and therefore show increased response times. This is not to argue against this particular model of behavioral inhibition, but just to say that our account may yield similar behaviors.

\subsection{Relation to Depression}

As an extension of approach-avoidance conflict, we next want to discuss the relationship between anxiety and depression. Anxiety and depression are highly comorbid, with roughly 45\% of individuals with a lifetime depression diagnosis also diagnosed with at least one anxiety disorder \citep{kessler2015}. Given this, many have clinical theories assume that depression and anxiety are slightly variant manifestations of the same underlying mechanisms involving negative affect (for review, see \cite{jacobson2014}).

Where the two disorders can be distinguished is in relation to rewards and anhedonia; depressive disorders, but not anxiety disorders, are associated with disrupted reward processing. One interesting proposal in the literature is that at least certain types of anhedonic depression are preceded by anxiety \citep{alloy1990, moitra2008, jacobson2014}. Specifically, persistent avoidance has the long-run effect of reducing the likelihood of anxious individuals encountering positive events and activities in their everyday lives. As a consequence, anxious individuals come to expect an absence of reward in their environments and subsequently develop diminished reward motivation and behavioral activation. Recent large-scale epidemiological studies have provided  some empirical support for this claim, finding that anxiety symptoms and behavioral avoidance precede and predict the longitudinal onset of depression (\cite{mathew2011, jacobson2014, kessler2015}; though also see \cite{jacobson2017, plana2019}).

\begin{figure}
  \centerline{%
    \resizebox{1.0\textwidth}{!}{\includegraphics[trim={0 0 0 0},clip]{../figures/06_lh.png}}%
  }
  \par \textbf{Figure 4:} Progression from anxiety to depression. In this open environment, an agent starts opposite a reward (blue) with harm separating them (A). For an optimistic agent ($w=1$), positive value spreads from the reward to the starting tile (Panels B-D, left-to-right). Harm is mitigated through the belief of future self-efficacy. For a pessimistic agent ($w=0.6$), both positive and negative value spread (Panels E-G, left-to-right). Given the agent's closer proximity to harm than reward, the optimal policy for this agent is to stay at the initial state. (Parameters: $\gamma = 0.95$)
\end{figure}

This prediction is easily accommodated by the present model. Simulations of how this might come back is presented in Figure 4. In this environment, there is only one rewarding and harmful state. Just as in the open field environment, the world is deterministic and the harmful state poses no true threat to an agent. An optimistic agent ($w=1$), with a belief in its ability to maintain an optimistic policy, is unfazed by the presence of the harmful state and develops over time an overall positive representation of the value of the environment (Figure 4b-d).

In contrast, a pessimistic agent ($w=0.6$) cannot be certain of its ability to steer clear of future harm. As such, negative value spreads from the harmful state to its antecedent states (Figure 4e-g). Because in this environment risk and reward are correlated (i.e. the agent must approach the harmful state in order to reach the reward given its starting position), the consequence of negatively spreading value is that the optimal policy is to stay in the initial position. Thus, the pessimistic agent adopts a behavioral policy akin to depressive immobility. Moreover, reward is inaccessible to this agent such that its positive expectations of the environment are also diminished. In summary, a loss of confidence in future reliability can result in behavioral policies that resemble depressive-like states in environments with correlated risk and reward.

\subsection{Planning}

In this next section, we review the literature on aversive pruning. In large, multi-step sequential decision environments, the space of all possible sequences of choices grows exponentially larger with increasing sequence length. As such, it is infeasible for decision making agents to exhaustively explore all possible sequences select that which maximizes reward from the entire set. Heuristics for narrowing the search space then are not only plausible but beneficial for a decision making agent.

One such heuristic that has been proposed in the literature is aversive pruning \citep{Huys2012}. Aversive pruning is defined as a Pavlovian response to encountering a large loss in planning such that sequences involving large losses are discarded from further evaluation. An example of such a scenario is presented in Figure 5. The prediction in this decision tree environment is that an agent would discard the branch of the decision tree involving an immediate large negative loss, even though this branch objectively contains the reward maximizing (loss minimizing) sequence of choices.

\cite{Huys2012} (and later \cite{Lally2017}) tested this prediction in this particular choice environment. Participants received extensive training in this decision tree environment (first without outcomes and then later with rewards) in order to facilitate planning. In free-planning trials, many participants exhibited this Pavlovian bias, rejecting choice sequences which involved the large loss even when the sequence was objectively reward maximizing (loss minimizing). Importantly \cite{Huys2012} and \cite{Lally2017} found the degree of aversive pruning was correlated with depressive and anxiety symptoms, respectively.

\begin{figure}
  \centerline{%
    \resizebox{1.0\textwidth}{!}{\includegraphics[trim={0 0 0 0},clip]{../figures/04_tree.png}}%
  }
  \par \textbf{Figure 5:} The decision tree environment from \cite{Huys2012} and \cite{Lally2017} (A). An optimistic agent ($w=1$) learns the optimal loss-minimizing policy through the initial large loss (B). A moderately pessimistic agent ($w=0.75$) will find both arms equally preferable insofar that it is less confident it will be able to execute optimal future choices (C). More pessimistic agents comes to prefer the branch without the large loss so as to avoid being unable to recoup the large initial loss (D).
\end{figure}

The aversive pruning principle makes tremendous sense and draws on a robust literature of heuristics and computational shortcuts for circumventing computationally intractable cognitive processes. It is important to note, however, that a control interpretation also makes similar predictions. Figure 5b-d shows the preferred routes of simulated agents in the same environments under increasingly pessimistic expectations of future actions. As can be observed, pessimistic agents are similarly likely to avoid the branch with the large loss, though for different reasons. Under the pessimistic case, the agent is increasingly less confident that the large gains later in sequence will be realized; as such, the agent is less confident that the initial large loss will be offset. Consequently, pessimistic agents would prefer the branch with smaller losses (even if this is objectively suboptimal). This prediction is similarly consistent with the finding that avoidance of the initial large loss is correlated with anxiety \cite{Lally2017}.

%% An interpretation like this would be consistent with reduced beliefs in one's own competency in such an environment. This is not strictly surprising. Planning multi-choice sequences even in relatively simple environments like those used in these experiments still ostensibly require working memory ability to keep in mind the possible consequences along each branch. Insofar that working memory ability is disrupted in anxiety disorders \citep{Moran2016}, a pessimistic belief may be justified and possibly optimal.

It is worth noting that these two alternate accounts can be disentangled through slight manipulations of the choice environment presented above. Aversive pruning and loss of confidence make similar predictions when a large loss begins a sequence, but not necessarily when a large potential loss comes later in a sequence. In the loss of confidence view, the potential for large loss late in a sequence negatively impacts the estimated value of actions early in the sequence; this is not true for aversive pruning. Identifying such choice sequences in the decision tree task may help identify whether this sort of inference is present in individuals with anxiety performing the task.

\subsection{Free Choice Premium}

In this final section, we propose based on the present theory that anxiety should be inversely related to the free choice bias. If an agent is confident in their future decision making abilities, then it follows that situations in which an individual is able to make a future choice should be preferable to situations in which an individual is not. At worst, an agent is no better off than if they had not made a choice; at best, an agent is better able to steer themselves toward desirable outcomes. Such is the logic that underlies an emerging literature suggesting that choice is inherently valuable \citep{Leotti2010}.

This free choice premium has been observed across several behavioral experiments \citep{Suzuki1997, Leotti2011, Leotti2014, Cockburn2014} using a variety of decision paradigms. For the present purposes, we will describe the experiments presented in \citep{Leotti2011, Leotti2014}. In these experiments, participants complete a two-stage decision making task (Figure 6). In the first stage, participants are make a choice between two cues: one leading to free choice and the other leading to a fixed choice. In the second stage, participants are able to choose between a second set of cues (free choice) or are forced to choose a cue (fixed choice). Unbeknownst to participants, all cues yield reward following the same outcome distribution; thus, no cue is advantageous in the long-run. The free choice premium is measured as a preference for the cue leading to free choice. \cite{Leotti2011, Leotti2014} find participants still prefer the free choice cue, despite this conferring no objective benefit to participants.

Insofar that the free choice premimum is contingent on an agent's belief in its future ability, it stands to reason that this free choice premium should be absent in individuals with anxiety and a loss of confidence. Simulations of this effect are presented in Figure 6. Simulated agents complete an experiment similar to that used in \cite{Leotti2011, Leotti2014}. In contrast to optimistic agents, the value of the worst action in the free choice arm back-propagated to the free-choice cue in pessimistic agents. With increasing pessimism then, the free choice option is less preferable than the fixed choice arm. As a result, pessimistic learners, on average, do not exhibit a bias towards the free choice arm. To our knowledge this prediction has not been empirically tested.

\begin{figure}
  \centerline{%
    \resizebox{1.0\textwidth}{!}{\includegraphics[trim={0 0 0 0},clip]{../figures/05_choice.png}}%
  }
  \par \textbf{Figure 6:} Schematic of the free choice bias tasks used in \cite{Leotti2011, Leotti2014} (A). The free choice bias is diminished in increasingly pessimistic agents (B).
\end{figure}

\section{Discussion}

Central to the acquisition and maintenance of anxiety disorders are symptoms including pessimistic inference, second-order conditioning, and persistent avoidance. In this article, we presented a simple computational account of these aberrations in learning and decision making. Specifically, we showed how a loss of confidence in the reliability of the future self and/or the environment can effectively backpropagate negative value across states of the environment. The result of this process are a series of inferences and behaviors resembling those observed in clinical anxiety. Though this is by no means a complete account of anxiety, the present model helps to explain the core symptoms of anxiety under one unifying framework.

Importantly, this computational account draws upon a longstanding recognition of the importance of perceived control to the development and maintenance of anxiety disorders. Central to many prominent theories of anxiety in the clinical literature highlight is a perceived lack of control. For example, learned helplessness theory and its successors claim that pathological anxiety results from a stable belief that the environment is uncontrollable, such that threat cannot be effectively mitigated. In contrast, Bandura's self-efficacy theory of anxiety posited that a lack of confidence in one's own ability to effectively handle threat was central to the pathogenesis of anxiety. Later theories, such as the triple vulnerability model, are equivocal as to whether an unreliable future environment or self is the root cause of anxiety, but highlights control beliefs as being a necessary compontent for developing anxiety. As we note above, the present model can accommodate either effect. Insofar that the state transition function and future state-action function are multiplicative, a pessimistic change to either should make similar predictions. It is possible that under certain conditions they may predict different outcomes. Future work may tease these apart, as they may be interesting for guiding treatment.

It is worth noting that we are not the first to attempt to formalize a theory of control in MDP. \cite{HuysDayan2009} provided a first computational account of learned helplessness through simple models of action-outcome contingencies. \cite{HuysDayan2009} found that training an agent first in an unreliable environment led to later impaired learning, akin to early learned helplessness experiments, by virtue of the prior of controllability. This provides a nice demonstration of learning impairment of action-outcome contingencies. Importantly, our accounts differ in at least three respects. First, we consider sequential decision environments, which is necessary for more accurately characterizing the symptoms of anxiety disorders. Second, our account specifies a mechanism by which the worst outcomes are realized; in their account, the world simply becomes random. This pessimism seems closer to what is observed in anxiety disorders. Finally, our account accounts for the persistence of these symptoms. Presumably, their model would eventually learn to escape with additional training; our account shows aberrant behaviors in the optimal (long-run) policy.

In this article, we discussed two mechanisms by which negative values can spread, through the state transition distribution and future state-action action. There is a third that we did not address here: that of the state itself. Here we assumed that states were fully observable; in other words, there was no uncertainty about the state an agent was currently occupying. In partially-observable Markov decision problems, the state itself must be inferred from external and internal variables. This is yet another means by which bad value can spread if undue likelihood is granted to undesirable or aversive states. This ideas has previously been proposed to explain some symptoms in anxiety \citep{Paulus2012}, and not without good reason. Anxiety disorders involve negatively biased interpretation of ambiguous percepts \citep{Hartley2012}, which may result in aberrant planning and increased avoidance for similar reasons to those described above. This is an area for future research.

In this article, we have explored a computational principle of approach and avoidance learning in sequential decision environments. We have not discussed, however, the particular psychological or neurobiological mechanisms by which this sort of learning may take place. This issue has been discussed in detail elsewhere \citep{Bishop2018}, but we briefly discuss a few possibilities here. One possibility is in biased sampling. If decision making and planning may rely on internal sampling from previously experienced episodes in order to make predictions about future value, then biased sampling may result in maladaptive decision making. Recent work has shown how finite sampling during planning can result in risk aversion and the overrepresentation of low probability but strongly negative outcomes (Lieder et al., 2018). Such a mechanism is prima facie in line with results showing the availability in memory of negative outcomes being higher for patients with anxiety \citep{Borkovec1999, Miranda2007}. Another possibility are an overreliance on Pavlovian heuristics, such as aversive pruning \citep{Huys2012,Lally2017}. As discussed above, future work should tease apart the predictions of our model and the aversive pruning heuristic.

Several recent studies have found evidence suggesting increased learning rates for negative reward prediction errors in individuals with elevated levels of anxiety \citep{Harle2017, Garrett2018, Aylward2019}. Though incearned learning from negative reward prediction errors may result in anxiety-like behaviors, these are likely not the sole cause of anxiety disorders for the reasons discussed above. Future research might investigate the relationship of prior beliefs about the reward statistics of the environment and learning to positive and negative prediction errors. A separate interestisng possibility is prioritized replay (Russek et al., 2017; Mattar and Daw, 2018). Offline replay of previous experiences (through hippocampal mechanisms) are known to facilitate learning. It has been proposed that biased replay could bias value estimates (Gagne et al., 2018).

\bibliographystyle{vancouver-authoryear}
\bibliography{citations}

\section{Appendix}

\begin{algorithm}
  \caption{Value Iteration}

  \State Algorithm parameter: a small threshold $\theta > 0$ determining accuracy of estimation
  \State Initialize $V(s)$, for all $s \in S$ arbitrarily, except that $V(terminal) = 0$
  \State
  \While{$\Delta > \theta$}
    \State $\Delta \leftarrow 0$
    \Loop \ for each $s \in S$
      \State $v \leftarrow V(s)$
      \State $ V(s) = \max_a \sum_{s',r} p(s',r|s,a) \left[ r + \gamma V(s') \right] $
      \State $\Delta \leftarrow \max(\Delta, |v - V(s)|)$
    \EndLoop
  \EndWhile

\end{algorithm}

\end{document}
