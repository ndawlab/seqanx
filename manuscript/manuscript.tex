\documentclass[11pt]{article} % For LaTeX2e
\usepackage{manuscript, palatino}
\usepackage{graphicx}
\usepackage{amsfonts, amsmath}
\usepackage{algorithm, algpseudocode}%

\usepackage{natbib}

\title{Anxiety: a decision-theoretic perspective}

\author{
Samuel Zorowitz \\
Princeton Neuroscience Institute\\
Princeton University\\
Princeton, NJ 08540 \\
\texttt{zorowitz@princeton.edu} \\
\And
Ida Momennejad \\
columbia University\\
New York, NY 10027 \\
\texttt{ida.m@columbia.edu} \\
\And
Nathaniel Daw \\
Princeton Neuroscience Institute\\
Princeton University\\
Princeton, NJ 08540 \\
\texttt{ndaw@princeton.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
To be filled in
\end{abstract}

\keywords{
reinforcement learning; avoidance; anxiety
}

\startmain

\section{Introduction}
Many psychiatric disorders involve distortions in models of the external world
and, correspondingly, aberrations of learning and decision making. As a normative
framework, decision theory allows us to describe and pose quantita­tive questions
about optimal and approximately optimal choice behavior \citep{DayanDaw2008} and
is, therefore, a critical tool for modeling, understanding, and predicting the
psychological and neurobiological mechanisms responsible for the abnormalities
observed in psychiatric behavior \citep{HuysDawDayan2015}. Many recent investigations
employing this approach have provided important insights into psychiatric symptoms,
including anhedonia in major depression \citep{Rutledge2017}, mood instability
in bipolar disorder \citep{EldarNiv2015, EldarDolanNiv2016},  habitual decision
making in disorders of compulsivity \citep{gillan2016}, and hallucinations in
schizophrenia \citep{powers2017, corlett2018}.

Anxiety disorders (generalized anxiety disorder, social anxiety disorder, panic
disorder, obsessive-compulsive disorder) represent the most common family of
psychiatric disorders (citation). Despite the differences in the content of worry,
all anxiety disorders share two core features: pessimistic inference and excessive
avoidance. Pessimistic inference describes a tendency to appraise threatening
situations as more frequent and/or more severe than they objectively are. This has
been reliably observed experimentally, as in fear conditioning wherein individuals
with anxiety exhibit exaggerated fear towards safe cues (CS-) and extinguished
threatening cues \citep{lissek2005, Duits2015}, and clinically, through self-report
(pick 3 citaitons). Excessive avoidance describes the corresponding tendency for
individuals with anxiety to consistently flee or avoid threatening situations.
Avoidance is often chosen even at the expense of potential reward (approch-avoidance
conflict), and is frequently a major impediment to recovery \citep{Arnaudova2017}.

Recent studies have attempted to provide some computational models of this behavior
\citep{Moutoussis2008, Maia2010}. For example, \citep{Maia2010} was able to show
how reinforcement learning could explain sustained avoidance in the absence of
repeated reinforcement, observed in rodents \citep{servatius2008} and something
not explained by two-factor theory \citep{Krypotos2015}. As has been discussed
elsewhere \citep{Moutoussis2017}, such accounts only explain the maintenance of
avoidance in short timescales, but do not explain why resistant to change and
why they do not go away in general. Such theories also do not explain the magnitude
of pessimistic inference, or the fear generalization. A broader account is needed.

In this article, we put forward a decision theoretic explanation of the core
symptoms of anxiety. In sequential decision environments, we show how a loss of
optimistic inference results in pessimistic inference, chronic avoidance, and
generalization of fear.

\section{Methods}

\subsection{Markov Decision Processes}

Here, we briefly review the formalism of Markov decision processes (MDPs), which
provide the foundation for our results. For more complete treatments of the
subject, see Sutton \& Barto (1998, 2018) and Bertsekas \& Tsitsiklis (1996).

A MDP is defined by a set of states, $S$, a set of actions, $A$, a reward
function, $R(s,a)$ over state/action pairs, and a state transition distribution,
$P(s'|s,a)$, where $a$ denotes a chosen action. States and rewards occur
sequentially according to these one-step functions, driven by a series of
actions; the goal is to learn to choose a probabilistic policy over actions,
denoted by $\pi$, that maximizes the value function, $V^\pi(s)$, defined as the
expected cumulative discounted reward:

$$ V^\pi(s) = \mathbb{E} \left[ \sum^{\infty}_{k=0} \gamma^k R_{t+k} | S_t = s \right] $$

Here, $\gamma$ is a parameter controlling temporal discounting. The value function
can also be defined recursively as the sum of the immediate reward of the action
chosen in that state, $R(s, a)$, and the value of its successor state $s’$,
averaged over possible actions, $a$, and transitions that would occur if the agent
chose according to $\pi$:

$$ V^\pi(s) = \sum_a \pi(a|s) \left[ R(s,a) + \sum_{s'} P(s'|s,a) \gamma V^\pi (S') \right] $$

The value function under the optimal policy is given by:

$$ V^*(s) = \max_a \mathbb{E} \left[ \sum^{\infty}_{k=0} \gamma^k R_{t+k} | S_t = s \right] $$

$$ = \max_a \left[ R(s,a) + \sum_{s'} P(s'|s,a) \gamma V^\pi (S') \right] $$

Knowledge of the value function can help to guide choices. For instance, we can
define the state-action value function as the value of choosing action $a$ and
following $\pi$ thereafter:

$$ Q^\pi(s,a) = \mathbb{E} \left[ \sum^{\infty}_{k=0} \gamma^k R_{t+k} | S_t = s \right] $$

$$ = R(s,a) + \sum_{s'} P(s'|s,a) \gamma V^\pi (S') $$

Then at any state one could choose the action that maximizes $Q^\pi(s,a)$. Formally
this defines a new policy, which is as good or better than the baseline policy
$\pi$; analogously, Eq 2 can be used to define the optimal state-action value
function, the maximization of which selects optimal actions. Note that it is
possible to write a recursive definition for $Q$ in the same manner as Eq 1, and
work directly with the state-action values, rather than deriving them indirectly
from $V$.

\subsection{Robust Control}

To create the symptoms of anxiety, we need a loss of control. This can come in
two forms. First is through the state transition distribution, $P(s'|s,a)$.
Normally, we assume the world is deterministic; in other words, an agent's chosen
action, $a$ leads the agent to the desired successor state, $s'$. In a stochastic
environment, however, an agent may instead transition to an undesired state, $s^*$.

Importantly, in the clinical literature, pathological anxiety is also associated
with a lack of perceived control (Gallagher et al., 2014). Studies suggest that
this belief mediates anxiety (ref?). This can come in whatever form.

Alternately, an agent may choose optimally in the present but may be uncertain of
its choice control in the future. In such a case, we deviate from the optimal
policy in the future. We can adopt a different function. There are a number to
choose from \citep{Garcia2015}. Here we adopt the beta-pessimism criterion
\citep{Gaskett2003}:

$$ V(s') = \beta \max_{a'} Q(s',a') + (1 - \beta) \min_{a'} Q(s',a') $$

where $\beta$ controls the degree of pessimism. When $\beta = 1$, an agent expects
complete control to choose the best action (best-case criterion); when $\beta = 0$,
an agent expects the  worst future case; and when $\beta = 0.5$, an agent expects
randomness. (We are not committed to this particular instantiation; it is simply
a convenient notation. Other parameterizations work equally well, see appendix.)

\subsection{Simulations}

In some instances, we solve for Q-values and state values directly through value
iteration. This is a form of Dynamic Programming. In other instances, we estimate
these terms through temporal difference learning. We use beta-pessimism (betamax)
learning for both. We use the former to solve for final (infinite horizon) values
and the latter to identify finite horizon values. We explain when we use both.

\section{Results}

\subsection{Pessimistic Inference}

One symptom central to all of the anxiety disorders is pessimistic inference,
or a tendency to appraise the likelihood and/or severity of threatening situations
out of proportion to their actual environmental contingencies \citep{dsm5, BeckClark1997,
ClarkBeck2011}. For example, an individual with specific phobia may vastly exaggerate
the probability of encountering their fear (e.g. a venomous spider) in their
everyday lives. The first studies of pessimistic inference in clinically anxious
populations involved individuals judging the perceived likelihood and severity
of different aversive situations, with the general finding that anxious individuals
rated these as more likely than non-clinical individuals \citep{ButlerMathews1983,
ButlerMathews1987, Foa1996, MacLeod1996, MacLeod1997, Luten1997, Stober1997,
Borkovec1999}; a finding that has been replicated more recently \citep{Maner2006,
Mitte2007, Miranda2007}.

Experimentally, aberrations in threat appraisal have been repeatedly observed
in a litany of associative learning studies. Common to all of these studies,
participants learn that a particular stimulus (CS+) predicts the occurrence
of a negative outcome (e.g. shock, loud noise, aversive image). In simple conditioning,
there is some evidence to suggest that individuals with anxiety exhibit elevated
levels of response to threatening stimuli \citep{lissek2005, MinekaOehlberg2008,
Duits2015}. Importantly, conditioning studies also suggest impaired safety learning
in anxiety, wherein a stimulus never paired with an aversive outcome (CS-) is also
feared \citep{Duits2015}. Other notable findings include impaired blocking,
conditioned discrimination (AX+/BX-) (citations), and conditioned inhibition
(A+, AB-) (citations).

% I think when you come back to edit this, you need to highlight these findings
% more in the context of what you are about to explain. Pessimistic inference
% is both making bad things worse and, most importantly, that non-bad things
% become bad. Although one question is how much to include the other less
% studied paradigms

\begin{figure}
  \centerline{%
    \resizebox{1.0\textwidth}{!}{\includegraphics[trim={0 0 0 0},clip]{../figures/01_field.png}}%
  }
  \caption{\textbf{Open Field}}
  \par Current parameters: Reward = 10, Shock = -10, gamma = 0.95.
\end{figure}

In our first result, we demonstrate one simple mechanism by which pessimistic
inference and exaggerated threat appraisals may arise. In the *open field* environment,
an agent is placed in a large grid world entirely empty except for two states,
leading to positive and negative outcomes, respectively. Under complete control
(objective or perceived), an agent in this environment in under no threat despite
the presence of negative states. This is simply because, in a fully controllable
world, an agent need not ever interact with the aversive state. In fact, an agent
can decide to walk straight up to the aversive state and, insofar it does not
choose to take one step further, will never be in any actual danger.

This is not so when deterministic control cannot be assumed. If an agent believes
it is not in full control of its environment, then it cannot rule out the possibility
that it may inadvertently encounter the negative state at some point in the future,
especially if it veers to close to the state. This is true irrespective of whether
the agent believes it cannot guarantee its own future competency or the benevolence
of the environment. Importantly, this belief need not match the true environmental
statistics; belief is enough.

The long-run state values under different beliefs are plotted in Figure 1. As
expected, under optimistic beliefs (Figure 1b) the entire environment sans negative
state takes on a positive value (close to the positive state itself). As this belief
breaks down, we observe a different pattern in the long-run state values. When
future occupancy of the negative state becomes a non-zero probability, the undesired
action transition exerts influence on action-outcome calculations, back-propagating,
and thereby infecting antecedent states. As the agent's belief becomes increasingly
pessimistic, the more the worst possible action exerts influence and the more
negative, on average, the state-space grows.

In the breakdown of perceived control then, negative states can grow worse (i.e.
more predictive of future negative outcome) and otherwise positive states can too.
This occurs as a simple result that if control not guaranteed, cannot guaranteed
bad things wont happen. 

\subsection{Avoidance}

1. Open with definition of avoidance. Follow \citep{Arnaudova2017} in definitions, i.e.
the basic idea that avoidance is the distancing of oneself from real or perceived
threat.
2. Describe cliff walking environment. Need to discuss that we are expanding
on the result originally described in \citep{Gaskett2003}.
3. With increasing pessimism, agent further distances self from cliff. Note that
this is the optimal policy under those set of beliefs about the world.
4. This can explain how avoidance is maintained. Even with luck and chance in the
the world, these sets of beliefs actually lead to and sustain these behavioral
policies.

In discussion of section, need to also mention other relevant findings. Specifically,
need to describe the approach-avoidance findings of Bach (2014, 2017) and the
complex task findings of Sheynin et al., 2015.

\begin{figure}
  \centerline{%
    \resizebox{1.0\textwidth}{!}{\includegraphics[trim={0 0 0 0},clip]{../figures/02_cliff.png}}%
  }
  \caption{\textbf{Cliff Walking}}
  \par Cliff = -100, Cost = -1, gamma = 0.99
\end{figure}

\subsection{Approach-Avoidance Conflict}

In the third section of the results, we discuss approach-avoidance conflict.
This covers tasks such as the balloon analog risk task (BART) and the flight
initiation distance task, which has been found to correlate with anxiety. The
hallmark of these sorts of tasks is the tradeoff between increased reward vs.
the possibility of an aversive outcome (e.g. balloon pop, predation). Discuss
previous findings, i.e. earlier leave times.

We can show that this is easily explained by our model. A lack of belief of
future control predicts exiting while things are still good. (We see this in
Figure 3). The optimal policy in these environments is an earlier leaving time.

This can also be used to explain behavioral inhibition \citep{bach2015, khemka2017}.
The idea is slightly different than what is advocated. The claim here is that
in anxious individuals, the Q-values should be closer together, and thus there
should be longer waiting time. This requires demonstrating that decisions are
not made in advance; only at the time of decision making. This is something
I think we will have to argue. (Need to double check the literature here.)

\begin{figure}
  \centerline{%
    \resizebox{1.0\textwidth}{!}{\includegraphics[trim={0 0 0 0},clip]{../figures/03_fid.png}}%
  }
  \caption{\textbf{Flight Initiation Distance}}
  \par remember to add parameters
\end{figure}

\subsection{Planning}

Basically reviewing the arguments of Huys here \citep{Huys2012, Lally2017}.
Need to get facts straight before going through all of this.

\begin{figure}
  \centerline{%
    \resizebox{1.0\textwidth}{!}{\includegraphics[trim={0 0 0 0},clip]{../figures/04_tree.png}}%
  }
  \caption{\textbf{Decision Tree}}
  \par remember to add parameters
\end{figure}

\subsection{Free Choice Premium}

\begin{figure}
  \centerline{%
    \resizebox{1.0\textwidth}{!}{\includegraphics[trim={0 0 0 0},clip]{../figures/05_choice.png}}%
  }
  \caption{\textbf{Free Choice Premium}}
  \par remember to add parameters
\end{figure}

\subsection{Longitudinal Progression}

\section{Discussion}

In this article, we showed...

Discuss previous computational approaches: This has also been previously been
investigated in previous computational modeling \citep{Huys2009}. There
the authors found that beliefs about a lack of controllability, mechanistically
modeled as corrupted priors about action-outcome contingencies, found deficient
learning in novel environments. The present results are no doubt related, but
address a different environment and a different behavior.

We discussed two mechanisms by which negative values can spread. There is a third
that we did not address here: that of the state itself. Here we assumed that states
were fully observable; in other words, there was no uncertainty about the state
an agent was currently occupying. In partially-observable Markov decision problems,
the state itself must be inferred from external and internal variables. This is
yet another means by which bad value can spread if undue likelihood is granted
to undesirable or aversive states. This ideas has previously been proposed to
explain some symptoms in anxiety \citep{Paulus2012}, and not without good reason.
Anxiety disorders involve negatively biased interpretation of ambiguous percepts
\citep{Hartley2012}, which may result in aberrant planning and increased avoidance
for similar reasons to those described above. This is an area for future research.

In this article, we have explored a computational principle of approach and
avoidance learning in sequential decision environments. We have not discussed,
however, the particular psychological or neurobiological mechanisms by which this
sort of learning may take place. This issue has been discussed in detail elsewhere
\citep{Bishop2018}, but we briefly discuss a few possibilities here. Mechanically,
we understand that complex decision problems are solved by mental processes that
exist on a spectrum between model-based and model-free (Daw et al., 2005). The
difference between these two families of methods hinges on the reliance of an
internal model of the world used to make predictions about expected long-run
outcomes of particular choices.

For model-based decision making, one possibility is biased planning. In large
state spaces, online sequential decision making suffers from the curve of
dimensionality: in planning a series of actions to take, comparing between
multiple options rapidly becomes computationally intractable when the number of
options and depth of search becomes even moderately large. Thus, heuristics for
reducing the size of this search can be useful. Biased pruning of search paths
(such as immediately discarding paths that require some minor loss in pursuit of
larger gains) may result in maladaptive decision making (Huys et al., 2012).
Alternately, if planning relies on internal sampling from previously experienced
episodes in order to make predictions about future value, then biased sampling
 may similarly result in maladaptive decision making. Recent work has shown how
 finite sampling during planning can result in risk aversion and the overrepresentation
 of low probability but strongly negative outcomes (Lieder et al., 2018). Such a
 mechanism is prima facie in line with results showing the availability in memory
 of negative outcomes being higher for patients with anxiety (Borkovec et al., 1999;
 Miranda and Mennin, 2007).

Within the realm of model-free decision making, several studies have found evidence
suggesting increased learning rates for negative reward prediction errors in
individuals with elevated levels of anxiety \citep{Aylward, Huang2017, Harle2017
garrett2018}. Though an asymmetry in sensitivity to
positive and negative reward prediction errors bears superficial similarity, for
the reasons described above merely attending more to negative information does not
necessarily predict increased avoidance insofar that avoidance can successfully
isolate negative outcomes. One possibility, and topic of future research, is how
prior beliefs about the reward statistics of the current environment dictate
learning rates. A second possibility is prioritized replay (Russek et al., 2017;
Mattar and Daw, 2018). Offline replay of previous experiences (through hippocampal
mechanisms) are known to facilitate learning. It has been proposed that biased
replay could bias value estimates (Gagne et al., 2018). This is another exciting
area for future research.


\section{References}

\bibliographystyle{vancouver-authoryear}
\bibliography{citations}

\section{Appendix}
\subsection{Algorithms}

\begin{algorithm}
  \caption{Value Iteration}

  \State Algorithm parameter: a small threshold $\theta > 0$ determining accuracy of estimation
  \State Initialize $V(s)$, for all $s \in S$ arbitrarily, except that $V(terminal) = 0$
  \State
  \While{$\Delta > \theta$}
    \State $\Delta \leftarrow 0$
    \Loop \ for each $s \in S$
      \State $v \leftarrow V(s)$
      \State $ V(s) = \max_a \sum_{s',r} p(s',r|s,a) \left[ r + \gamma V(s') \right] $
      \State $\Delta \leftarrow \max(\Delta, |v - V(s)|)$
    \EndLoop
  \EndWhile

\end{algorithm}

\begin{algorithm}
  \caption{Betamax Temporal Difference Learning}

  \State Algorithm parameters: step size $\eta \in (0, 1)$, small $\epsilon > 0$
  \State Initialize $Q(s,a)$, for all $s \in S$, $a \in A$ arbitrarily, except that $Q(terminal,\cdot) = 0$
  \State
  \Loop \ for each episode
    \State Initialize $S$
    \While{$S \notin S(terminal)$}
      \State Choose $A$ from $S$ using policy derived from $Q$
      \State Take action $A$, observe $R, S$
      \State $Q(S, A) \leftarrow Q(S, A) + \eta \left[ R + \gamma \max_a Q(S',a) − Q(S, A) \right] $
      \State $S \leftarrow S'$
    \EndWhile
  \EndLoop

\end{algorithm}

\end{document}
